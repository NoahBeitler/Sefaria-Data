from typing import List
import django, argparse, json, zipfile, re
django.setup()
from functools import reduce, partial
from tqdm import tqdm
from dataclasses import dataclass
from collections import defaultdict
from sefaria.model import *
from sources.local_settings import DICTA_LIBRARY_PATH
from sources.functions import post_index, post_text, post_link_in_steps, add_term

"""
TODO
category, en title, section names, version title, version source
"""

VERSION_NOTES = """This text was digitized and released into the public domain by <a href="https://dicta.org.il">Dicta: The Israel Center for Text Analysis</a>, using a state-of-the-art OCR pipeline leveraging a custom-built transformer-based language model for Rabbinic Hebrew. Nikud (Vocalization) marks were auto-generated by <a href="https://nakdanlive.dicta.org.il">Dicta's Nakdan system</a>."""
DICTA_PARALLELS_INDEX_NAME = "Dicta Parallels"

def create_simple_index(en_title, he_title, categories, section_names, address_types=None, add_collective=False):
    root = JaggedArrayNode()
    root.add_primary_titles(en_title, he_title)
    root.add_structure(section_names, address_types=address_types)
    root.key = en_title
    root.validate()
    if add_collective:
        add_term(en_title, he_title)
    index = {
        "title": en_title,
        "categories": categories,
        "schema": root.serialize(),
    }
    if add_collective:
        index['collective_title'] = en_title
    return index


class DictaLibraryManager:

    def __init__(self, lib_path):
        self.lib_path = lib_path
        self.books_by_name = {}
        self.parallels = []  # list of all parallels in all parsed books
        with open(f"{self.lib_path}/books.json", "r") as fin:
            self.books_list = [DictaBook(lib_path=DICTA_LIBRARY_PATH, **book_dict) for book_dict in json.load(fin)]
        for book in self.books_list:
            self.books_by_name[book.displayName] = book

    def get_book(self, name):
        return self.books_by_name[name]

    def parse_and_post_book(self, book_name: 'DictaBook', server, post=False):
        book = self.get_book(book_name)
        self.parallels += book.parse()
        if post:
            book.post(server)

    def create_and_post_parallels_commentary(self, server, post=False):
        index = create_simple_index(DICTA_PARALLELS_INDEX_NAME, "המקבילות של דיקטה", ["Reference"], ["Chapter", "Paragraph"], ["Integer", "Integer"], add_collective=True)
        links = []
        parallels_text = []
        for ipage, page_parallels in enumerate(self.parallels):
            parallels_text += [[]]
            for iparallel, parallel in enumerate(page_parallels):
                parallels_text[-1] += [parallel.serialize()]
                parallel_tref = f'{index["title"]} {ipage+1}:{iparallel+1}'
                links += [{
                    "refs": [parallel.base_tref, parallel_tref],
                    "type": "commentary",
                    "auto": True,
                    "generated_by": "dicta library parse",
                    "inline_reference": {
                        "data-order": parallel.dataOrder,
                        "data-commentator": DICTA_PARALLELS_INDEX_NAME,
                        "data-label": parallel.dataLabel,
                    }
                }]
        version = {
            "text": parallels_text,
            "versionTitle": "Dicta Library",
            "versionSource": "https://library.dicta.org.il",
            "language": "he",
            "versionNotes": VERSION_NOTES
        }
        post_index(index, server=server)
        post_text(index['title'], version, server=server, skip_links=True)
        post_link_in_steps(links, 100, 1, server=server, skip_lang_check=True)


@dataclass
class DictaParallel:
    base_tref: str
    baseTextLength: int
    baseStartChar: int
    baseMatchedText: str
    sources: list
    tool: str
    baseStartToken: int = None
    baseEndToken: int = None
    dataOrder: int = None
    dataLabel: str = None

    def serialize(self):
        base_text = self.baseMatchedText
        best_comp = self.__best_comp()
        comp_text = best_comp['compDisplayText']
        comp_name = best_comp.get('compNameHe', best_comp.get('verseDispHe', ''))
        if not comp_name:
            print(best_comp)
        return f"<b>{base_text[:1000]}</b><br><small>{comp_text[:1000]}<br>({comp_name})</small>"

    def __best_comp(self):
        sorted_comps = sorted(self.sources, key=lambda x: len(x.get('compMatchedText', '')))
        return sorted_comps[-1]

@dataclass
class DictaPage:
    displayName: str
    fileName: str
    nakdanResponseFile: str

    def parse(self, root_dir, book_title):
        jin = self.__get_json_content(root_dir)
        text = self.__get_text(jin)
        paragraphs = text.split('\n')
        index = self.__get_zero_based_index()
        parallels = self.__get_parallels(jin, book_title, index, paragraphs)
        paragraphs = self.__add_parallel_markers_to_text(paragraphs, parallels)
        return paragraphs, index, parallels

    def __add_parallel_markers_to_text(self, paragraphs: List[str], parallels: List[DictaParallel]) -> str:
        from bisect import bisect_right
        parallels.sort(key=lambda x: x.baseStartChar)
        parag_end_indexes = reduce(
            lambda a, b: a + [len(b) + ((a[-1] + 1) if len(a) > 0 else 0)],
            paragraphs, []
        )
        char_offset_by_paragraph = defaultdict(int)
        for ipar, par in enumerate(parallels):
            seg_index = bisect_right(parag_end_indexes, par.baseStartChar)
            data_order = ipar + 1
            data_label = data_order  # TODO for now
            marker = f"""<i data-commentator="{DICTA_PARALLELS_INDEX_NAME}" data-label="{data_label}" data-order="{data_order}"></i>"""
            temp_text = paragraphs[seg_index]
            seg_offset = (parag_end_indexes[seg_index-1] + 1) if seg_index > 0 else 0
            temp_start = par.baseStartChar + char_offset_by_paragraph[seg_index] - seg_offset
            temp_text_left = temp_text[temp_start:temp_start + min(len(par.baseMatchedText), len(temp_text) - temp_start)]
            assert temp_text_left == par.baseMatchedText[:len(temp_text_left)]
            paragraphs[seg_index] = f"{temp_text[:temp_start]}{marker}{temp_text[temp_start:]}"
            char_offset_by_paragraph[seg_index] += len(marker)
            par.dataOrder = data_order
            par.dataLabel = data_label
        return paragraphs

    def __get_zero_based_index(self):
        m = re.search(r'\d+$', self.displayName)
        return int(m.group(0)) - 1

    def __get_json_content(self, root_dir):
        json_fname = self.fileName.replace('.zip', '.json')
        with zipfile.ZipFile(f"{root_dir}/{self.fileName}") as zin:
            with zin.open(json_fname, mode='r') as fin:
                return json.load(fin)

    @staticmethod
    def __get_text(jin):
        return reduce(lambda a, b: a + b['str'], jin['tokens'], "")

    @staticmethod
    def __get_parallels(jin: dict, book_title: str, page_index: int, paragraphs: List[str]) -> List[DictaParallel]:
        parallels = filter(lambda x: x and x['tool'] == 'parallels', jin['data']['postProcessedSources'])
        return list(map(partial(DictaPage.__create_parallel_object, book_title, page_index, paragraphs), parallels))

    @staticmethod
    def __create_parallel_object(book_title:str, page_index: int, paragraphs: List[str], parallel_dict: dict):
        from bisect import bisect_right
        base_text_match = parallel_dict['baseMatchedText']
        parag_end_indexes = reduce(
            lambda a, b: a + [len(b) + ((a[-1] + 1) if len(a) > 0 else 0)],
            paragraphs, []
        )
        start_seg_index = bisect_right(parag_end_indexes, parallel_dict['baseStartChar'])
        end_seg_index = bisect_right(parag_end_indexes, parallel_dict['baseStartChar'] + len(base_text_match))

        # testing
        segs_with_base_text = " ".join(paragraphs[start_seg_index: end_seg_index + 1])
        assert base_text_match in segs_with_base_text
        if start_seg_index < end_seg_index:
            # move start forward
            segs_wo_base_text = " ".join(paragraphs[start_seg_index+1: end_seg_index+1])
            #assert base_text_match not in segs_wo_base_text
            # move end backward
            segs_wo_base_text = " ".join(paragraphs[start_seg_index:end_seg_index])
            #assert base_text_match not in segs_wo_base_text

        # base tref
        seg_str = f"{start_seg_index+1}" if start_seg_index == end_seg_index else f"{start_seg_index+1}-{end_seg_index+1}"
        base_tref = f"{book_title} {page_index+1}:{seg_str}"
        return DictaParallel(
            base_tref=base_tref,
            **parallel_dict
        )

@dataclass
class DictaBook:
    lib_path: str
    displayName: str
    fileName: str
    printYear: int
    printLocation: str
    author: str
    category: str
    categoryEnglish: str = None
    authorEnglish: str = None
    printLocationEnglish: str = None
    displayNameEnglish: str = None
    source: str = None
    firstpage: dict = None  # seems to be a non-standard field
    pages: List[DictaPage] = None

    def __post_init__(self):
        self._root_path = f"{self.lib_path}/{self.fileName}"
        self._parsed_pages = []
        self.__load_pages()

    def __load_pages(self):
        try:
            with open(f"{self._root_path}/pages.json", "r") as fin:
                self.pages = [DictaPage(**page_dict) for page_dict in json.load(fin)]
        except FileNotFoundError:
            print(f"No directory for {self.displayName}")

    def parse(self):
        self.__create_index()
        parallels = self.__create_version()
        return parallels

    def post(self, server):
        post_index(self.index, server=server)
        post_text(self.index['title'], self.version, server=server, skip_links=True)

    def __create_index(self):
        self.index = create_simple_index(self.displayNameEnglish, self.displayName, ["Responsa"], ["Daf", "Paragraph"], address_types=['Integer', 'Integer'])

    def __create_version(self):
        parsed_pages = []
        all_parallels = []
        for page in tqdm(self.pages, desc=self.fileName):
            paragraphs, index, parallels = page.parse(self._root_path, self.index['title'])
            all_parallels += [list(parallels)]
            while len(parsed_pages) < index:
                parsed_pages += [[]]
                # all_parallels += [[]] TODO currently don't require parallels to have same pagination as base text
            parsed_pages += [paragraphs]
        self.version = {
            "text": parsed_pages,
            "versionTitle": "Dicta Library",
            "versionSource": "https://library.dicta.org.il",
            "language": "he",
            "versionNotes": VERSION_NOTES
        }
        return all_parallels


def init_argparse() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument("book", help='name of book to parse')
    parser.add_argument("-s", "--server", dest="server", help="server to post to")
    return parser


if __name__ == '__main__':
    parser = init_argparse()
    args = parser.parse_args()
    dicta = DictaLibraryManager(DICTA_LIBRARY_PATH)
    dicta.parse_and_post_book(args.book, args.server, post=True)
    dicta.create_and_post_parallels_commentary(args.server, post=True)

# https://dicta-library.cauldron.sefaria.org

"""
TODO get rid of empty paragraphs but make sure start chars are correct
TODO see if we can get citations working
TODO add commentary markers
"""