import django, re, requests, json
django.setup()
from research.quotation_finder.dicta_api import *
import pymongo
from sefaria.settings import *
import cProfile
from functools import reduce
from sefaria.tracker import modify_bulk_text
client = pymongo.MongoClient(MONGO_HOST, MONGO_PORT)  # (MONGO_ASPAKLARIA_URL)
db_qf = client.quotations

# read book segment links from DB (or while posting?)
# read book segment textChunk from DB
# find place in seg (CharLevelData)
# paste in the pasukRef in normal hebrew.

dummy_char = "█"
only_prefixed = True


class QuotationLink(link.Link):

    collection = 'quotations'
    # from_file = base_file_dict

    required_attrs = [
        "type",             # string of connection type
        "refs",             # list of refs connected
        "base_text",        # text of the ref in the link that we are looking to insert the inline citation in
        "tanakh_oref",      # although these are easily retrieved form the refs attribute it is best that they are
                            # spelled out for each function once
        "tanakh_tref",
    ]

    optional_attrs = [
        "expandedRefs0",    # list of refs corresponding to `refs.0`, but breaking ranging refs down into individual segments
        "expandedRefs1",    # list of refs corresponding to `refs.1`, but breaking ranging refs down into individual segments
        "anchorText",       # string of dibbur hamatchil (largely depcrated)
        "availableLangs",   # list of lists corresponding to `refs` showing languages available, e.g. [["he"], ["he", "en"]]
        "highlightedWords", # list of strings to be highlighted when presenting a text as a connection
        "auto",             # bool whether generated by automatic process
        "generated_by",     # string in ("add_commentary_links", "add_links_from_text", "mishnah_map")
        "source_text_oid",  # oid of text from which link was generated
        "is_first_comment", # set this flag to denote its the first comment link between the two texts in the link
        "first_comment_indexes", # Used when is_first_comment is True. List of the two indexes of the refs.
        "first_comment_section_ref", # Used when is_first_comment is True. First comment section ref.
        "inline_reference",  # dict with keys "data-commentator" and "data-order" to match an inline reference (itag)
        "charLevelData",     # list of length 2. Containing 2 dicts coresponding to the refs list, each dict consists of the following keys: ["startChar","endChar","versionTitle","language"]. *if one of the refs is a Pasuk the startChar and endChar keys are startWord and endWord. This attribute was created for the quotation finder
        "score",             # int. represents how "good"/accurate the link is. introduced for quotations finder
        "inline_citation",    # bool acts as a flag for wrapped refs logic to run on the segments where this citation is inline.
        "trivial_ref"
    ]  # link.Link.optional_attrs.extend(new_optional_attrs)

    def __init__(self, link_d, from_file=''):
        link = Link(link_d)
        self.refs = link.refs
        self.type = link.type
        self.score = link.score
        self.charLevelData = link.charLevelData
        self.tanakh_tref = link.refs[0]
        self.tanakh_oref = Ref(self.tanakh_tref)
        assert self.tanakh_oref.index.title in library.get_indexes_in_category("Tanakh")
        self.tc = TextChunk(Ref(self.refs[1]), 'he')
        self.seg_text = get_seg_text(self.refs[1], from_file=from_file)
        self.seg_text_list = list(re.sub('\s+', dummy_char, self.seg_text))
        if hasattr(link, 'trivial_ref'):
            self.trivial_ref = link.trivial_ref
        self.dh = link.dh if hasattr(link, 'dh') else False


    def load(self, query, proj=None):
        obj = getattr(db_qf, self.collection).find_one(query, proj)
        if obj:
            assert set(obj.keys()) <= set(self._saveable_attr_keys()), \
                "{} record loaded with unhandled key(s): {}".format(
                    type(self).__name__,
                    set(obj.keys()) - set(self._saveable_attr_keys())
                )
            self.load_from_dict(obj, True)
            return self
        return None  # used to check for existence of record.

    def to_post(self):
        post_dict = {
            "type": self.type,
            "refs": self.refs,
            "auto": True,
            "charLevelData": self.charLevelData,
            "score": self.score,
            "generated_by": "quotation_finder"
        }
        if post_dict["type"] == "quotation_auto":
            post_dict.update({"inline_citation": True})
        elif post_dict["type"] == "dibur_hamatchil":
            post_dict.update({"inline_citation": False})
        return post_dict

    def distence_to_dh(self, l_dh):
        if not l_dh:
            return 100
        else:
            dis = self.tanakh_oref.distance(l_dh.tanakh_oref)
            return 100 if dis == -1 else dis


def get_links_for_citation_insert(ref, score, link_source, from_file=''):
    """

    :param ref:
    :param score:
    :param link_source:
    :return: list of links created by the dicta_api code.
    """
    if isinstance(link_source, dict):  # link_source == dict
        # lls = [Link(l) for l in link_source if ref.normal() in l['refs'] and l['score'] >= score]
        lls = [Link(l) for l in link_source.get(ref.normal(), [{'score': -1}]) if l['score'] >= score]
        return lls if len(lls) > 0 else None
    elif link_source == 'quotation_DB':
        query = {"refs": ref.normal(), "score": {"$gte": score}}
        cursor = db_qf.quotations.find(query)
        # cursor_len =  db_qf.quotations.count_documents(query)
        links = [QuotationLink(l, from_file=from_file) for l in list(cursor)]
        links = dict([(link.charLevelData[1]['startChar'], link) for link in links]).values() # this line is to eliminate duplicates.
        return links if list(links) != [] else None#if cursor_len > 0 else None
    else:  # link_source == 'Sefaria_DB'
        return list(LinkSet({"refs": ref.normal(), "type": "quotation_auto", "score": {"$gte": score}}))


def get_seg_text(tref, from_file=None):
    seg_text = ''
    if from_file:
        seg_text = from_file.get(tref, "")  # todo: this should change the obj file as well
    else:
        tc = Ref(tref).text('he')
        if tc.version():
            tc.vtitle = tc.version().versionTitle  # this makes some sense because it is always on a single segment
            seg_text = tc.text
    if not seg_text:
        print(f"missing text in seg {tref}")
    return seg_text


def get_segment(ref, score=22, link_source=None, from_file=None, prefix_char_range=30):
    """

    :param ref: oref
    :param score:
    :param link_source:
    :param from_file:
    :param prefix_char_range:
    :return:
    """
    seg_text = get_seg_text(ref.normal(), from_file=from_file)
    seg_text_list = list(re.sub('\s+', dummy_char, seg_text))
    # pat = re.compile('^(<.*?>)(.*?)(<.*?>)')
    # html_dh = re.match(pat, seg_text)
    move=0
    # if html_dh:
    #     seg_text_list = list(re.sub('\s+', dummy_char, re.sub(pat, html_dh.group(2), seg_text)))
    #     move = len(html_dh.groups(1))+len(html_dh.groups(3))
    lls = get_links_for_citation_insert(ref, score, link_source, from_file=from_file)

    if lls:
        text_w_citations, final_links = add_citations(lls, seg_text_list, ref.normal(), prefix_char_range=prefix_char_range)
        # print(f"check {link2url(lls[0], sandbox='quotations')}")
    else:
        return {}, []
    return {ref.normal(): text_w_citations}, final_links


def get_place(link, seg_text_list):
    place = link.charLevelData[1]['endChar']
    search_here = ''.join(seg_text_list[place:min(place+10, len(seg_text_list))]).replace(dummy_char, ' ')
    match_gomer = re.match(""".*?(וגו|וכו)('|"|מר)?.*?""", search_here)
    if match_gomer:
        move = len(match_gomer.group())
        place = place + move
    return place, match_gomer


def sheneemar(link, seg_text_list, prefix_char_range=10):
    place = link.charLevelData[1]['startChar']
    search_here = ''.join(seg_text_list[max(place-prefix_char_range, 0):place+1]).replace(dummy_char, ' ')
    match_sheneemar = re.match(""".*?(וזהו|ש?נ?אמר|(ד|ו)?כתיב|ו?כתוב|ו?הדר).*?""", search_here)
    if match_sheneemar:
        return True
    return False

def test_word_char_level(link, citation):
    pasuk_chars = list(re.sub('\s+', dummy_char, Ref(link.refs[0]).text('he').text))
    start =link.charLevelData[0]['startWord']
    end = link.charLevelData[0]['endWord']
    words_list = pasuk_chars[start: end]
    words = ''.join(words_list)
    words = words.replace(dummy_char, ' ')
    citation = f'{citation}<sup>{words}</sup>'

    #test words in base text
    #
    # start = l.charLevelData[1]['startChar']
    # end = l.charLevelData[1]['endChar']
    # words = ''.join(seg_text_list[start:end]).replace(dummy_char, " ")
    # citation = f'{citation}<sup>{words}</sup>'
    return citation


def get_citation(link, color_score=None):
    # the following 2 assumptions about the placing in the refs list and the charLevelData list are based on the link testing\and changing if needed in add_citations link. we can make here another assertion but that seems redundant
    pasuk_ref = Ref(link.refs[0])
    # place = link.charLevelData[1]['endChar']
    citation = f"{dummy_char}({pasuk_ref.normal('he')})"  # {dummy_char}"
    # if color_score:
    #     if color_score[0] < link.score < color_score[1]:
    #         color = 1  # orange
    #     elif color_score[1] <= link.score < color_score[2]:
    #         color = 2  # green
    #     elif color_score[2] <= link.score:
    #         color = 3  # blue
    #     else:
    #         return ''
    #     citation = f'<span class="ref-link-color-{color}">{citation}</span>'
        # citation = f'<sup><span class="ref-link-color-{color}">*</span></sup><i class="footnote">{citation}</i>'
    # citation = test_word_char_level(link, citation)
    return (citation, link)


def to_be_embedded(ls, lls, seg_text_list, book_ref):
    """
    This function trims the links to imbed inline based on the inline links that are already present. shams. and trivial inline citation heuristics.
    :param ls: linkSet on this segment prior to quotations
    :param lls: linkSet of quotation finder (the set to be trimmed)
    :param seg_text_list: segment text
    :param book_ref: tref of the segment
    :return: left_links, text_to_delete: left_links: are the quotation links to be embedded. text_to_delete. (for shams and other text deletion in future)
    """

    toRef = lambda l: l.refs[0] if l.refs[1] == book_ref else l.refs[1]
    inline_citation_links_there = [toRef(l) for l in ls if hasattr(l, 'inline_ciation') and l.inline_citation]
    tc = list(lls)[0].seg_text  #Ref(book_ref).text('he').text  # todo: shouldn't be reading form local!
    # wrapped_text = library.get_wrapped_refs_string(tc)  # todo: this is very heavy line.
    # wrapped = re.findall('(<a.*?data-ref="(.*?)".*?a>)', wrapped_text)
    # wrapped_trefs = [wrap[1] for wrap in wrapped]
    refs_in_db = [l for l in ls if l.type != "quotation_auto"]
    trivial = [toRef(l) for l in refs_in_db if l.type == 'commentary'] + [toRef(l) for l in lls if hasattr(l, "trivial")]
    quotation_refs = [toRef(l) for l in lls]
    def_not_needed = set(inline_citation_links_there).intersection(set(quotation_refs))
    l_dh_list = [l for l in lls if l.dh]
    l_dh = None if not l_dh_list else l_dh_list[0]
    left_links = [l for l in lls if l.tanakh_tref not in def_not_needed and l.tanakh_tref not in trivial and l.distence_to_dh(l_dh) > 1]#[l for l in lls if toRef(l) not in def_not_needed and toRef(l) not in trivial and l.tanakh_tref.distence(l.dh)]
    text_to_delete = []
    for l in left_links:
        startPlace = l.charLevelData[1]['startChar']
        before = ')' in seg_text_list[startPlace-5:startPlace]
        if before:
            inline_options = re.findall("[(\[](.*)[)\]]", tc[startPlace-20:startPlace])
            for par in inline_options:
                if "שם" in par:
                    text_to_delete.append(par)
                    left_links.remove(l)
                    continue
                else:
                    print(f"{par} : {l.refs}")
                    db_qf.humaneye.insert_one({"refs": l.refs, "parenthesis": par})
        endPlace = l.charLevelData[1]['endChar']
        after = '(' in seg_text_list[endPlace:endPlace+5]
        if after:
            inline_options = re.findall("[(\[](.*)[)\]]", tc[startPlace+20:startPlace])
            for par in inline_options:
                if "שם" in par:
                    text_to_delete.append(par)
                    left_links.remove(l)
                else:
                    print(f"{par} : {l.refs}")
                    db_qf.humaneye.insert_one({"refs": l.refs, "parenthesis": par})
    return left_links, text_to_delete


def delete_square_close_citations(citation_list, seg_dis = 150, verse_dis = 4):
    """
    Use the citation place in segment, and the verse place in Tanakh to decide if there is need to trim further the inline citations
    :param citation_list: list of tuples [(p1,c1),(p2,c2)...(pn,cn)] (place in segment, citation)
    :return: citation_list as above but trimmed
    """
    delete = []
    citation_places = [e[0] for e in citation_list]
    citation_places.sort()
    R = lambda e: Ref(re.sub(f'[)({dummy_char}]', '', e[1][0]))
    for e in citation_list:
        close_space_range = range(e[0], e[0]+seg_dis)
        close_space = [c for c in citation_list if c[0] in close_space_range if c!=e]
        for c in close_space:
            if 0 <= R(c).distance(R(e)) < verse_dis:
                delete.append(c)
    new_citation_list = [item for item in citation_list if item not in delete]
    return new_citation_list


def add_citations(lls, seg_text_list, book_ref, prefix_char_range=30):
    """

    :param lls: list of the links to be added in inline citations. (according to links that were created by quotation_finder)
    :param seg_text_list:
    :param book_ref:
    :return:
    """
    citation_list =[]
    final_links = []
    cnt = 0
    ls = LinkSet(Ref(book_ref))
    trimmed_lls, _ = to_be_embedded(ls, lls, seg_text_list, book_ref)
    for l in trimmed_lls:
        # if l.charLevelData[1]['startChar'] <= 10 or (hasattr(l, 'dh') and l.dh):  # check for DH
        if l.type == 'dibur_hamatchil':
            # post_link(l.contents())
            final_links.append(l)
            db_qf.quotations.update_one({"charLevelData": f"{l.charLevelData}", "refs": f"{l.refs}"},
                                    {"$set": {"post": True}})
            continue
        if Ref(l.refs[1]).book != Ref(book_ref).book:
            print("needed reverse")
            l.refs.reverse()
            l.charLevelData.reverse()
        # todo: before placing the citation in, check what the text looks like, are there other citations or shamas inline already? is this a dh? or is there a diffrenet reason we don't want to add this l link?
          # or do we want to add it but in a different place?
        # seg_citation_list = get_place_citation(l, color_score=[22, 30, 50])
        place, vego = get_place(l, seg_text_list)
        citation = get_citation(l, color_score=[22,30,50])
        prefixed = sheneemar(l, seg_text_list, prefix_char_range=prefix_char_range) if only_prefixed else None
        if prefixed:
            db_qf.quotations.update_one({"charLevelData": f"{l.charLevelData}", "refs": f"{l.refs}"},
                                    {"$set": {"prefixed": True}})
        if only_prefixed and not (prefixed or vego) and not hasattr(l, "prefixed"):
            continue
        seg_citation_list = [(place, citation)]
        citation_list.extend(seg_citation_list)
    citation_list = delete_square_close_citations(citation_list)
    citation_list.sort()
    # if move and citation_list:
    #     citation_list_n = [(cl[0]+move, cl[1]) for cl in citation_list]
    #     citation_list = citation_list_n
    for p, c in citation_list:
        place = p+cnt
        seg_text_list.insert(place, c[0])
        cnt += 1
        final_links.append(c[1])
    text = ''.join(seg_text_list)
    text = re.sub(f'{dummy_char}+', ' ', text)
    # text = re.sub('(\(.*?\))(\s+)(\.|,)', '\g<1>\g<3>', text)
    # print(text)
    return text, final_links


def push_text_w_citations(vtitle, vsource, new_texts_dict, server = SEFARIA_SERVER):
    for r, t in new_texts_dict.items():
        # create_payload_and_post_text(r, t, 'he', vtitle, vsource, server=server)
        tc = TextChunk(Ref(r), 'he', vtitle=vtitle)
        tc.text = t
        tc.save()


def get_local_seg(ref):
    return {ref.normal(): ref.text('he').text}


def order_links_by_segments(links): #, base_book_title):
    from collections import defaultdict
    link_dict = defaultdict(list)
    pasuk_books = library.get_indexes_in_category('Tanakh')
    for l in links:
        if Ref(l['refs'][1]).book in pasuk_books:
            key_ref = l['refs'][0]
        else:
            # assert base_book_title in l['refs'][1]
            key_ref = l['refs'][1]
        link_dict[key_ref].append(l)
    return link_dict


def modify_text_localy(title, version, new_texts_dict, new_version=None, server=SEFARIA_SERVER):
    v = version
    if v == None:
        text_1_dict = dict([list(new_texts_dict.items())[0]])
        r = Ref(title)
        versionSource = r.first_available_section_ref().text('he').version().versionSource
        push_text_w_citations(version, versionSource, text_1_dict, server=server)
    modify_bulk_text(UID, v, new_texts_dict, skip_links=True)
    return


def get_links_to_post_not_to_embed(base_ref, score=25):
    query = {"refs": base_ref.normal(), "score": {"$gte": score}}
    cursor = db_qf.quotations.find(query)
    links = [QuotationLink(l) for l in list(cursor)]
    final_links = []
    for l in links:
        if l.type == 'dibur_hamatchil':
            final_links.append(l.to_post())
        if l.tanakh_tref == l.trivial_ref:
            final_links.append(l.to_post())
    return final_links


if __name__ == '__main__':
    title = "Tzror HaMor on Torah"
    ref_title = f"Tzror_HaMor_on_Torah, Genesis.17.15.1"
    cat = "tanakh_comm"
    new_texts_dict = dict()
    all_links = []
    range_ref = Ref(ref_title)  # 'Yalkut_Shimoni_on_Torah.783.3' 1.1-161.6, "Kinnot for Tisha B'Av (Ashkenaz), Kinot for Tisha B'Av Night"
    path = os.getcwd()
    base_file = f'{path}/offline/text_mappings/{cat}/{title}.json'
    base_file_dict = get_from_file(base_file)
    v = Version().load({'title': 'Tzror HaMor on Torah', 'versionTitle': 'Tzeror Hamor, Warsaw, 1879'})#Ref(library.get_index(title).get_first_ref_in_base_text(title)).text('he').version()
    # links = get_links_from_file('Ramban_on_Numbers.31.23.json')
    # link_dict = order_links_by_segments(links)  #,"Selichot_Nusach_Ashkenaz_Lita")
    min_score = 25
    cnt=0
    for r in range_ref.all_segment_refs(): #base_file_dict.keys():
        # r = Ref(ref)
        # cProfile.run('''get_segment(r, score=min_score, link_source='quotation_DB', prefix_char_range=30)''')
        text_dict, links = get_segment(r, score=min_score, link_source='quotation_DB', prefix_char_range=30, from_file=base_file_dict)
        new_texts_dict.update(text_dict) #, prefix_char_range=30   from_file=get_from_file("Tzror_HaMor_on_Torah.json"), prefix_char_range=30))
        all_links.extend([l.to_post() for l in links])
        only_sidebar_links = get_links_to_post_not_to_embed(r, score=min_score)
        all_links.extend(only_sidebar_links)
        cnt+=1
        print(f"next {cnt}")
        # new_texts_dict.update(get_local_seg(r))
    # push_text_w_citations(v.versionTitle, v.versionSource, new_texts_dict)
    # push_text_w_citations('test squareclose prefix 25', v.versionSource, new_texts_dict)
    # modify_bulk_text(UID, Version().load({'title': "Kinnot for Tisha B'Av (Ashkenaz), Kinot for Tisha B'Av Night", 'versionTitle': 'test30'}), new_texts_dict)
    vtitle = v.title #f'test_minscore_{min_score}'
    # new_version_title = f'test_minscore_{min_score}'
    # new_version = {
    #         'versionTitle': new_version_title,
    #         'versionSource': 'citations',
    #         'language': 'he',
    #         'text': ''
    #     }
    try:
        post_link(all_links, server="http://localhost:8000")
    except ConnectionError:
        pass
    modify_text_localy(range_ref.index.title, v, new_texts_dict, server="http://localhost:8000")

    # push_text_w_citations(v.versionTitle, v.versionSource, new_texts_dict)
    # post_text()